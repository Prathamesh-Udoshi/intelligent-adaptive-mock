"""
Learning Service
=================
Core logic for processing the learning buffer, storing drift alerts,
health metrics, and managing the request log.
"""

import datetime
import logging
from typing import List, Dict

from sqlalchemy import select
from sqlalchemy.orm.attributes import flag_modified
from sqlalchemy.ext.asyncio import AsyncSession

from core.database import AsyncSessionLocal
from core.state import (
    LEARNING_BUFFER, LEARNING_BUFFER_SIZE, RECENT_LOGS,
    buffer_lock, logs_lock, health_monitor
)
from core.websocket import manager
from core.models import Endpoint, EndpointBehavior, ChaosConfig, ContractDrift, HealthMetric
from utils.schema_intelligence import learn_and_compare

logger = logging.getLogger("mock_platform")


# ‚îÄ‚îÄ Endpoint Lookup / Creation ‚îÄ‚îÄ

async def get_or_create_endpoint(session: AsyncSession, method: str, path_pattern: str):
    """
    Find an existing endpoint or create a new one.
    NOTE: Does NOT call session.commit() ‚Äî the caller is responsible for committing.
    
    Safe against race conditions: if two coroutines try to create the same endpoint
    concurrently, the second will catch the IntegrityError and re-fetch the row.
    """
    from core.state import TARGET_URL
    from sqlalchemy.exc import IntegrityError

    result = await session.execute(
        select(Endpoint).where(Endpoint.method == method, Endpoint.path_pattern == path_pattern)
    )
    endpoint = result.scalars().first()

    if not endpoint:
        try:
            endpoint = Endpoint(method=method, path_pattern=path_pattern, target_url=TARGET_URL)
            session.add(endpoint)
            await session.flush()  # Get the ID without committing

            behavior = EndpointBehavior(endpoint_id=endpoint.id)
            chaos = ChaosConfig(endpoint_id=endpoint.id)
            session.add(behavior)
            session.add(chaos)
            await session.flush()
        except IntegrityError:
            # Another coroutine created this endpoint concurrently ‚Äî roll back the
            # duplicate attempt and fetch the row that the winner inserted.
            await session.rollback()
            result = await session.execute(
                select(Endpoint).where(Endpoint.method == method, Endpoint.path_pattern == path_pattern)
            )
            endpoint = result.scalars().first()

    return endpoint


# ‚îÄ‚îÄ Logging ‚îÄ‚îÄ

async def add_to_logs(method: str, path: str, status: int, latency: int, type: str, has_drift: bool = False, health_info: dict = None):
    """Append a log entry and broadcast to WebSocket clients."""
    async with logs_lock:
        log_entry = {
            "time": datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),  # UTC ISO ‚Äî browser localises it

            "method": method,
            "path": path,
            "status": status,
            "latency": round(latency),
            "type": type,
            "has_drift": has_drift,
            "health": health_info.get("status", "healthy") if health_info else "healthy",
            "health_score": health_info.get("health_score", 100) if health_info else 100
        }
        RECENT_LOGS.insert(0, log_entry)
        if len(RECENT_LOGS) > 50:
            RECENT_LOGS.pop()

    # Broadcast to all dashboard clients
    broadcast_data = {"type": "update", "data": log_entry}
    if health_info and health_info.get("anomalies"):
        broadcast_data["health_alert"] = health_info
    broadcast_data["global_health"] = health_monitor.get_global_health()
    await manager.broadcast(broadcast_data)


# ‚îÄ‚îÄ Background Tasks ‚îÄ‚îÄ

async def store_drift_alert(endpoint_id: int, drift_score: float, drift_summary: str, drift_details: List[Dict]):
    """Stores or updates a contract drift alert in the database. Prevents duplicates."""
    try:
        async with AsyncSessionLocal() as session:
            res = await session.execute(
                select(ContractDrift).where(
                    ContractDrift.endpoint_id == endpoint_id,
                    ContractDrift.is_resolved == False
                ).order_by(ContractDrift.detected_at.desc())
            )
            existing_alerts = res.scalars().all()

            if existing_alerts:
                existing = existing_alerts[0]
                existing.detected_at = datetime.datetime.utcnow()
                existing.drift_score = drift_score
                existing.drift_summary = drift_summary
                existing.drift_details = drift_details

                # AUTO-CLEANUP orphaned duplicates
                if len(existing_alerts) > 1:
                    for orphaned in existing_alerts[1:]:
                        orphaned.is_resolved = True
                        orphaned.resolved_at = datetime.datetime.utcnow()
                    logger.info(f"üßπ Cleaned up {len(existing_alerts)-1} orphaned alerts for endpoint {endpoint_id}")

                logger.info(f"üîÑ Updated existing drift alert for endpoint {endpoint_id}")
            else:
                drift_alert = ContractDrift(
                    endpoint_id=endpoint_id,
                    drift_score=drift_score,
                    drift_summary=drift_summary,
                    drift_details=drift_details
                )
                session.add(drift_alert)
                logger.info(f"üö® New drift alert stored for endpoint {endpoint_id}")

            await session.commit()
    except Exception as e:
        logger.error(f"‚ùå Failed to store/update drift alert: {str(e)}")


async def store_health_metric(endpoint_id: int, latency_ms: float, status_code: int, response_size: int, health_result: dict):
    """Background task: stores a health metric snapshot in the database."""
    try:
        async with AsyncSessionLocal() as session:
            metric = HealthMetric(
                endpoint_id=endpoint_id,
                latency_ms=latency_ms,
                status_code=status_code,
                response_size_bytes=response_size,
                is_error=status_code >= 400,
                latency_anomaly=health_result.get("latency_anomaly", False),
                error_spike=health_result.get("error_spike", False),
                size_anomaly=health_result.get("size_anomaly", False),
                health_score=health_result.get("health_score", 100.0),
                anomaly_reasons=[a["message"] for a in health_result.get("anomalies", [])]
            )
            session.add(metric)
            await session.commit()
            logger.info(f"üìà Health metric stored for endpoint {endpoint_id} (Score: {health_result.get('health_score')})")
    except Exception as e:
        logger.error(f"‚ùå Failed to store health metric: {str(e)}")


# ‚îÄ‚îÄ Learning Buffer Processor ‚îÄ‚îÄ

async def process_learning_buffer():
    """Process accumulated traffic observations into learned behaviors."""
    import core.state as state

    async with buffer_lock:
        if len(state.LEARNING_BUFFER) < LEARNING_BUFFER_SIZE:
            return
        batch = state.LEARNING_BUFFER[:]
        state.LEARNING_BUFFER.clear()  # MUST use .clear(), not = [] (would break proxy.py's reference)

    # Process each item in its OWN session to avoid cross-contamination
    for item in batch:
        try:
            method = item['method']
            path_pattern = item['path_pattern']
            status = item['status']
            latency = item['latency']
            resp_body = item['response_body']
            req_body = item['request_body']

            behav_found = False
            async with AsyncSessionLocal() as session:
                async with session.begin():  # Atomic transaction per item
                    # Find endpoint (or create it within this transaction)
                    result = await session.execute(
                        select(Endpoint).where(
                            Endpoint.method == method,
                            Endpoint.path_pattern == path_pattern
                        )
                    )
                    endpoint = result.scalars().first()
                    behavior = None

                    if not endpoint:
                        # proxy.py always creates the endpoint before scheduling this
                        # background task, so this path should never be reached in
                        # normal operation. If it happens (e.g. the endpoint was deleted
                        # between the proxy request and this background flush), skip
                        # rather than creating a duplicate.
                        logger.warning(
                            f"‚ö†Ô∏è process_learning_buffer: endpoint {method} {path_pattern} "
                            f"not found in DB ‚Äî skipping this observation."
                        )
                        continue
                    else:
                        # Only SELECT for existing endpoints ‚Äî for new ones we already have the object
                        b_res = await session.execute(
                            select(EndpointBehavior).where(EndpointBehavior.endpoint_id == endpoint.id)
                        )
                        behavior = b_res.scalars().first()

                    if not behavior:
                        logger.warning(f"‚ö†Ô∏è No behavior row for {method} {path_pattern}, skipping.")
                    else:
                        behav_found = True
                        # ‚îÄ‚îÄ Latency (snap on first real observation) ‚îÄ‚îÄ
                        alpha = 0.5
                        if behavior.latency_mean >= 399.9:  # Still at default 400ms
                            behavior.latency_mean = round(latency, 2)
                        else:
                            behavior.latency_mean = round(
                                (behavior.latency_mean * (1 - alpha)) + (latency * alpha), 2
                            )

                        # ‚îÄ‚îÄ Status Code Distribution ‚îÄ‚îÄ
                        status_str = str(status)
                        if not behavior.status_code_distribution:
                            new_dist = {status_str: 1.0}
                        else:
                            new_dist = dict(behavior.status_code_distribution)
                            for k in list(new_dist.keys()):
                                new_dist[k] = round(new_dist[k] * (1 - alpha), 6)
                            new_dist[status_str] = round(new_dist.get(status_str, 0.0) + alpha, 6)
                            total = sum(new_dist.values())
                            new_dist = {k: round(v / total, 6) for k, v in new_dist.items()}
                        behavior.status_code_distribution = new_dist
                        flag_modified(behavior, "status_code_distribution")

                        # ‚îÄ‚îÄ Error Rate ‚îÄ‚îÄ
                        is_error_sample = 1.0 if status >= 400 else 0.0
                        if behavior.error_rate == 0.0 and is_error_sample > 0:
                            behavior.error_rate = round(is_error_sample, 4)
                        else:
                            behavior.error_rate = round(
                                (behavior.error_rate * (1 - alpha)) + (is_error_sample * alpha), 4
                            )

                        # ‚îÄ‚îÄ Schema Learning (Schema Intelligence Engine) ‚îÄ‚îÄ
                        if status < 300 and resp_body and isinstance(resp_body, (dict, list)):
                            # learn_and_compare updates the SchemaRegistry (persisted to disk)
                            # AND returns the rich schema for storing in the DB behavior record
                            new_schema, changes = learn_and_compare(
                                f"{method} {path_pattern}",  # keyed by "METHOD /path" for uniqueness
                                resp_body
                            )
                            behavior.response_schema = new_schema
                            flag_modified(behavior, "response_schema")
                            logger.info(f"üìã Response schema captured for {method} {path_pattern}")
                            if changes:
                                breaking = [c for c in changes if c["severity"] == "BREAKING"]
                                if breaking:
                                    logger.warning(f"üö® BREAKING schema change on {method} {path_pattern}: {breaking[0]['path']}")
                        else:
                            logger.debug(f"‚è≠Ô∏è  Schema skip: status={status}, body_type={type(resp_body).__name__}, body_truthy={bool(resp_body)}")

                        if req_body and isinstance(req_body, (dict, list)):
                            req_schema, _ = learn_and_compare(
                                f"REQ {method} {path_pattern}",
                                req_body
                            )
                            behavior.request_schema = req_schema
                            flag_modified(behavior, "request_schema")
                            logger.info(f"üìã Request schema captured for {method} {path_pattern}")

                        session.add(behavior)
                        # session.begin() auto-commits on clean exit

            logger.info(f"‚úÖ Learned: {method} {path_pattern} | latency={latency:.0f}ms | status={status}")

        except Exception as e:
            logger.error(f"‚ùå Error learning from {item.get('method')} {item.get('path_pattern')}: {str(e)}")
            continue

    logger.info(f"üìÅ Processed learning batch of {len(batch)} item(s).")
